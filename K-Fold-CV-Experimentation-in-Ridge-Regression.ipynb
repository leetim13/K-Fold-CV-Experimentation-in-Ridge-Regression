{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K Fold CV Experimentation on Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this mini project is to experiment with the `K-fold cross validation (CV)` procedure to tune the penalty parameter `λ` in Ridge regression, without using any pre-defined packages from libraries like `scikit-learn`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background on Ridge Regression and `MAP` estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In linear regression, we suppose that $\\mathbf{X} \\in \\mathbf{R}^{n\\times m}$ with $n ≥ m$ and $t ∈ \\mathbf{R}^n$, and that  $\\mathbf{t}|(\\mathbf{X}, \\mathbf{w}) \\sim \\mathbf{N}(\\mathbf{Xw}, \\sigma^2\\mathbf{I})$. We know that the maximum likelihood estimate $\\hat{\\mathbf{w}}$ of $\\mathbf{w}$ is given by $\\hat{\\mathbf{w}} = (X^TX)^{-1}X^Tt$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, suppose we place a normal prior on $\\mathbf{w}|\\mathbf{X}$, i.e., $\\mathbf{w} \\sim \\mathbf{N}(0,\\tau^2\\mathbf{I})$. \n",
    "Then, the `MAP` (Maximum a posteriori) estimate  of $\\mathbf{w}$ is given as the maximum of the posterior density:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\hat{\\mathbf{w}}_{MAP} = \\underset{\\mathbf{w}}{\\text{argmax}}\\{p(\\mathbf{w}|\\mathbf{X}, \\mathbf{t}) \\propto p(\\mathbf{t}|\\mathbf{X}, \\mathbf{w})p(\\mathbf{w}|\\mathbf{X})\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Note that we use the $\\propto$ notation here to mean \"proportional to\", since we dropped the term $p(t|X)$ in the denominator as it doesn’t have $w$ in it, and thus doesn't contribute to the maximization problem.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By taking the log-likelihood and setting the derivative to 0, the `MAP` estimate of $w$ given $(t, X)$ will be given as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\hat{\\mathbf{w}}_{MAP} = (X^TX+\\lambda I)^{-1}X^Tt$$  where $\\lambda = \\frac{\\sigma^2}{\\gamma^2}$. We will use this as the coefficients of the ridge regression with penalty level $\\lambda$ in the implementation of CV below with the function `train_model`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First we will set aside a test dataset and never use it until the training and parameter tuning procedures are complete. We will use this data for final evaluation. Test data is used as a separate dataset.\n",
    "2. Use the CV error to estimate the test error of a particular hyperparameter choice. For a particular hyperparameter value, we will split the training data into K blocks, and for k = 1, 2, ..., K we will use the k-th block for validation and the remaining K − 1 blocks for training. Therefore, we will train and validate our algorithm K times. The CV estimate for the test error for that particular hyperparameter choice is given by the average validation error across these K blocks.\n",
    "3. Repeat the above procedure for several hyperparameter choices and choose the one that provides us with the smalles CV error (which is an estimate for the test error)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `data` we will use is a variable and refers to a (`t`, `X`) pair (in test, training, or validation sets) where `t` is the target (response) vector, and `X` is the feature matrix.\n",
    "`model` is a variable and refers to the coefficients of the trained model, i.e. $\\hat{w}_{\\lambda}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `shuffle_data(data)` will take data as an argument and returns its randomly permuted version along the samples. We will use a uniformly random permutation of the training data and note that `t` and `X` will be permuted the same way preserving the target-feature pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_data(data):\n",
    "    np.random.shuffle(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `split_data(data, num_folds, fold)` is a function that takes data, number of partitions as `num_folds` and the selected partition fold as its arguments and returns the selected partition (block) fold as `data_fold`, and the remaining data as `data_rest`. In other words, if we consider 5-fold cross validation, `num_folds=5`, the function will split the data into 5 blocks and returns the block $fold \\in {1, 2, 3, 4, 5})$ as the validation fold and the remaining 4 blocks as data_rest. Note that `data_rest` $\\cup$ `data_fold = data`, and `data_rest` $\\cap$ `data_fold` $= \\emptyset$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data, num_folds, fold): #data is [(2, array([2,3,4])), array((b, [1,2,3]))] #fold = 1,2,3..\n",
    "    data_rest = []\n",
    "    data_fold = []\n",
    "    slicing = len(data) / num_folds\n",
    "    for i in range(len(data)):\n",
    "        if i != (fold): \n",
    "            data_rest.extend(data[int(i*slicing):int((i+1)*slicing)])\n",
    "        else:\n",
    "            data_fold.extend(data[int(i*slicing):int((i+1)*slicing)])\n",
    "    return data_fold, data_rest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `train_model(data, lambd)` takes data and lambd as its arguments, and returns the coefficients of ridge regression with penalty level $λ$, using the expression we derived above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(data, lambd):\n",
    "    X = []\n",
    "    t = []\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        X.append(data[i][1])\n",
    "        t.append(data[i][0])\n",
    "    X = np.array(X)\n",
    "    t = np.array(t)\n",
    "    X_transpose = X.transpose()\n",
    "    return (np.linalg.inv(X_transpose.dot(X) + lambd * np.identity(X.shape[1])).dot(X_transpose)).dot(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `predict(data, model)` is a function that takes data and model as its arguments, and returns the predictions based on data and model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data, model):\n",
    "    X = []\n",
    "    t = []\n",
    "    for i in range(len(data)):\n",
    "        X.append(data[i][1])\n",
    "        t.append(data[i][0])\n",
    "    X_np = np.array(X)\n",
    "    return np.dot(X_np, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `loss(data, model)` takes data and model as its arguments and returns the average squared error loss based on model. This means if data is composed of $t \\in R^n$ and $X \\in R^{n \\times p}$ and the model is $\\hat{\\mathbf{w}}$ , then the return value will be $||t - X\\hat{\\mathbf{w}}||^2/n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(data, model):\n",
    "    X = []\n",
    "    t = []\n",
    "    n = len(data)\n",
    "    for i in range(len(data)):\n",
    "        X.append(data[i][1])\n",
    "        t.append(data[i][0])\n",
    "    X = np.array(X)\n",
    "    t = np.array(t)\n",
    "    prediction = predict(data, model)\n",
    "    error = (np.linalg.norm(t - prediction ,2) **2)/n\n",
    "    return error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `cross_validation(data, num_folds, lambd_seq)` takes the training data, number of folds num_folds, and a sequence of $\\lambda$'s as `lambd_seq` as its arguments and returns the cross validation error across all $\\lambda$'s. If we take `lambd_seq` as evenly spaced 50 numbers over the interval (0.02, 1.5). This means `cv_error` will be a vector of 50 errors corresponding to the values of `lambd_seq`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(data, num_folds, lambd_seq):\n",
    "    data = shuffle_data(data)\n",
    "    cv_error = []\n",
    "    for i in range(len(lambd_seq)):\n",
    "        lambd = lambd_seq[i]\n",
    "        cv_loss_lmd = 0\n",
    "        for fold in range(num_folds):\n",
    "            val_cv, train_cv = split_data(data, num_folds, fold)\n",
    "            model = train_model(train_cv, lambd)      \n",
    "            cv_loss_lmd += loss(val_cv, model)\n",
    "        cv_error.append(cv_loss_lmd / num_folds)\n",
    "    return cv_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup\n",
    "import matplotlib. pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "data_train = {'X': np.genfromtxt('data_train_X.csv', delimiter= ','),\n",
    "                                 't': np.genfromtxt('data_train_y.csv', delimiter=',')}\n",
    "data_test = {'X': np.genfromtxt('data_test_X.csv', delimiter= ','),\n",
    "                                 't': np.genfromtxt('data_test_y.csv', delimiter=',')}\n",
    "\n",
    "t_train = data_train['t']\n",
    "X_train = data_train['X']\n",
    "t_test = data_test['t']\n",
    "X_test = data_test['X']\n",
    "\n",
    "data_train_ = []\n",
    "for i in range(len(t_train)):\n",
    "    data_train_.append((t_train[i], X_train[i]))\n",
    "\n",
    "data_test_ = []\n",
    "for i in range(len(t_test)):\n",
    "    data_test_.append((t_test[i], X_test[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize a `lambd_seq` amd find the training and test errors corresponding to each $\\lambda$ in `lambd_seq`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02      , 0.05020408, 0.08040816, 0.11061224, 0.14081633,\n",
       "       0.17102041, 0.20122449, 0.23142857, 0.26163265, 0.29183673,\n",
       "       0.32204082, 0.3522449 , 0.38244898, 0.41265306, 0.44285714,\n",
       "       0.47306122, 0.50326531, 0.53346939, 0.56367347, 0.59387755,\n",
       "       0.62408163, 0.65428571, 0.6844898 , 0.71469388, 0.74489796,\n",
       "       0.77510204, 0.80530612, 0.8355102 , 0.86571429, 0.89591837,\n",
       "       0.92612245, 0.95632653, 0.98653061, 1.01673469, 1.04693878,\n",
       "       1.07714286, 1.10734694, 1.13755102, 1.1677551 , 1.19795918,\n",
       "       1.22816327, 1.25836735, 1.28857143, 1.31877551, 1.34897959,\n",
       "       1.37918367, 1.40938776, 1.43959184, 1.46979592, 1.5       ])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambd_seq = np.linspace (0.02 , 1.5 , num =50)\n",
    "lambd_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error is 0.049736491547443556 for lamda = 0.02\n",
      "testing error is 5.106959783590839 for lamda = 0.02\n",
      "\n",
      "training error is 0.10583474384162299 for lamda = 0.050204081632653066\n",
      "testing error is 3.6308336211216363 for lamda = 0.050204081632653066\n",
      "\n",
      "training error is 0.1539702959791624 for lamda = 0.08040816326530613\n",
      "testing error is 3.070316730406794 for lamda = 0.08040816326530613\n",
      "\n",
      "training error is 0.19754784437778394 for lamda = 0.11061224489795919\n",
      "testing error is 2.770942353158946 for lamda = 0.11061224489795919\n",
      "\n",
      "training error is 0.23812998498606192 for lamda = 0.14081632653061224\n",
      "testing error is 2.5873531089834767 for lamda = 0.14081632653061224\n",
      "\n",
      "training error is 0.2765778425897501 for lamda = 0.1710204081632653\n",
      "testing error is 2.46600554123685 for lamda = 0.1710204081632653\n",
      "\n",
      "training error is 0.3134077787636036 for lamda = 0.20122448979591837\n",
      "testing error is 2.3821345226047588 for lamda = 0.20122448979591837\n",
      "\n",
      "training error is 0.34894854455537194 for lamda = 0.23142857142857143\n",
      "testing error is 2.322604952044621 for lamda = 0.23142857142857143\n",
      "\n",
      "training error is 0.383419726293373 for lamda = 0.2616326530612245\n",
      "testing error is 2.2797682611372783 for lamda = 0.2616326530612245\n",
      "\n",
      "training error is 0.4169740335581198 for lamda = 0.2918367346938776\n",
      "testing error is 2.2488554532446563 for lamda = 0.2918367346938776\n",
      "\n",
      "training error is 0.44972145002038727 for lamda = 0.32204081632653064\n",
      "testing error is 2.2267329040044137 for lamda = 0.32204081632653064\n",
      "\n",
      "training error is 0.4817436850928968 for lamda = 0.3522448979591837\n",
      "testing error is 2.2112542627296 for lamda = 0.3522448979591837\n",
      "\n",
      "training error is 0.5131031658916717 for lamda = 0.38244897959183677\n",
      "testing error is 2.2008990424758874 for lamda = 0.38244897959183677\n",
      "\n",
      "training error is 0.5438488213771094 for lamda = 0.41265306122448986\n",
      "testing error is 2.1945597160259633 for lamda = 0.41265306122448986\n",
      "\n",
      "training error is 0.5740199114355471 for lamda = 0.4428571428571429\n",
      "testing error is 2.191410495310007 for lamda = 0.4428571428571429\n",
      "\n",
      "training error is 0.6036486259742307 for lamda = 0.47306122448979593\n",
      "testing error is 2.1908233267429944 for lamda = 0.47306122448979593\n",
      "\n",
      "training error is 0.6327618883816958 for lamda = 0.503265306122449\n",
      "testing error is 2.19231234559038 for lamda = 0.503265306122449\n",
      "\n",
      "training error is 0.6613826315442856 for lamda = 0.5334693877551021\n",
      "testing error is 2.19549610907252 for lamda = 0.5334693877551021\n",
      "\n",
      "training error is 0.6895307165151994 for lamda = 0.5636734693877552\n",
      "testing error is 2.2000712857772213 for lamda = 0.5636734693877552\n",
      "\n",
      "training error is 0.7172236043300794 for lamda = 0.5938775510204082\n",
      "testing error is 2.2057939302127725 for lamda = 0.5938775510204082\n",
      "\n",
      "training error is 0.744476854302519 for lamda = 0.6240816326530613\n",
      "testing error is 2.212465901009173 for lamda = 0.6240816326530613\n",
      "\n",
      "training error is 0.7713044984197196 for lamda = 0.6542857142857144\n",
      "testing error is 2.2199248420992013 for lamda = 0.6542857142857144\n",
      "\n",
      "training error is 0.7977193260052297 for lamda = 0.6844897959183673\n",
      "testing error is 2.2280366793638153 for lamda = 0.6844897959183673\n",
      "\n",
      "training error is 0.8237331025510497 for lamda = 0.7146938775510204\n",
      "testing error is 2.236689923866469 for lamda = 0.7146938775510204\n",
      "\n",
      "training error is 0.8493567396836511 for lamda = 0.7448979591836735\n",
      "testing error is 2.2457912928224277 for lamda = 0.7448979591836735\n",
      "\n",
      "training error is 0.874600428464105 for lamda = 0.7751020408163266\n",
      "testing error is 2.2552623053593632 for lamda = 0.7751020408163266\n",
      "\n",
      "training error is 0.899473744902703 for lamda = 0.8053061224489797\n",
      "testing error is 2.265036608704424 for lamda = 0.8053061224489797\n",
      "\n",
      "training error is 0.9239857342239819 for lamda = 0.8355102040816327\n",
      "testing error is 2.2750578581743093 for lamda = 0.8355102040816327\n",
      "\n",
      "training error is 0.9481449787418323 for lamda = 0.8657142857142858\n",
      "testing error is 2.28527802162468 for lamda = 0.8657142857142858\n",
      "\n",
      "training error is 0.9719596529922097 for lamda = 0.8959183673469389\n",
      "testing error is 2.295656012489221 for lamda = 0.8959183673469389\n",
      "\n",
      "training error is 0.9954375688851004 for lamda = 0.9261224489795918\n",
      "testing error is 2.3061565795529306 for lamda = 0.9261224489795918\n",
      "\n",
      "training error is 1.0185862129836332 for lamda = 0.9563265306122449\n",
      "testing error is 2.3167493990436494 for lamda = 0.9563265306122449\n",
      "\n",
      "training error is 1.041412777531398 for lamda = 0.986530612244898\n",
      "testing error is 2.3274083274349833 for lamda = 0.986530612244898\n",
      "\n",
      "training error is 1.063924186483408 for lamda = 1.016734693877551\n",
      "testing error is 2.338110782861745 for lamda = 1.016734693877551\n",
      "\n",
      "training error is 1.0861271175193965 for lamda = 1.0469387755102042\n",
      "testing error is 2.3488372301764078 for lamda = 1.0469387755102042\n",
      "\n",
      "training error is 1.1080280208071303 for lamda = 1.0771428571428572\n",
      "testing error is 2.359570750067329 for lamda = 1.0771428571428572\n",
      "\n",
      "training error is 1.1296331351214224 for lamda = 1.1073469387755104\n",
      "testing error is 2.3702966767743354 for lamda = 1.1073469387755104\n",
      "\n",
      "training error is 1.1509485017993362 for lamda = 1.1375510204081634\n",
      "testing error is 2.38100229210287 for lamda = 1.1375510204081634\n",
      "\n",
      "training error is 1.1719799769147732 for lamda = 1.1677551020408163\n",
      "testing error is 2.3916765658918995 for lamda = 1.1677551020408163\n",
      "\n",
      "training error is 1.1927332419795647 for lamda = 1.1979591836734695\n",
      "testing error is 2.402309935006742 for lamda = 1.1979591836734695\n",
      "\n",
      "training error is 1.213213813418383 for lamda = 1.2281632653061225\n",
      "testing error is 2.4128941144341827 for lamda = 1.2281632653061225\n",
      "\n",
      "training error is 1.233427051017561 for lamda = 1.2583673469387755\n",
      "testing error is 2.4234219352488062 for lamda = 1.2583673469387755\n",
      "\n",
      "training error is 1.2533781655104115 for lamda = 1.2885714285714287\n",
      "testing error is 2.4338872051679505 for lamda = 1.2885714285714287\n",
      "\n",
      "training error is 1.2730722254317446 for lamda = 1.3187755102040817\n",
      "testing error is 2.444284588171958 for lamda = 1.3187755102040817\n",
      "\n",
      "training error is 1.2925141633503363 for lamda = 1.3489795918367347\n",
      "testing error is 2.454609500277588 for lamda = 1.3489795918367347\n",
      "\n",
      "training error is 1.3117087815688546 for lamda = 1.3791836734693879\n",
      "testing error is 2.464858019046957 for lamda = 1.3791836734693879\n",
      "\n",
      "training error is 1.3306607573652036 for lamda = 1.4093877551020408\n",
      "testing error is 2.475026804816476 for lamda = 1.4093877551020408\n",
      "\n",
      "training error is 1.3493746478366544 for lamda = 1.439591836734694\n",
      "testing error is 2.4851130319587362 for lamda = 1.439591836734694\n",
      "\n",
      "training error is 1.3678548943978908 for lamda = 1.469795918367347\n",
      "testing error is 2.4951143287599042 for lamda = 1.469795918367347\n",
      "\n",
      "training error is 1.3861058269757283 for lamda = 1.5\n",
      "testing error is 2.5050287247173055 for lamda = 1.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_data = shuffle_data(data_train_)\n",
    "testing_data = shuffle_data(data_test_)  \n",
    "training_error_list = []\n",
    "testing_error_list = []\n",
    "\n",
    "five_fold_error = cross_validation(training_data, 5, lambd_seq)\n",
    "ten_fold_error = cross_validation(training_data, 10, lambd_seq)\n",
    "\n",
    "for lambd in lambd_seq:\n",
    "    model = train_model(training_data, lambd)\n",
    "    training_error = loss(training_data, model)\n",
    "    training_error_list.append(training_error)\n",
    "    testing_error = loss(testing_data, model)\n",
    "    testing_error_list.append(testing_error)\n",
    "\n",
    "    print(\"training error is \"+ str(training_error) + \" for lamda = \" +str (lambd))\n",
    "    print(\"testing error is \"+ str(testing_error) + \" for lamda = \" +str (lambd)+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot training error, test error, and 5-fold and 10-fold cross validation errors on the same plot for each value in `lambd_seq`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lambd_seq, training_error_list, label='training_error') \n",
    "plt.plot(lambd_seq, testing_error_list, label='testing_error')\n",
    "plt.plot(lambd_seq, five_fold_error, label='five_fold_error') \n",
    "plt.plot(lambd_seq, ten_fold_error, label='ten_fold_error')\n",
    "plt.ylabel(\"Error\")\n",
    "plt.xlabel(\"lambda\")\n",
    "plt.legend()\n",
    "plt.savefig(\"final_plot.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda value proposed by 5-fold cv is: 0.38244897959183677 having a cv error of 2.8770597183858024\n",
      "Lambda value proposed by 10-fold cv is: 0.32204081632653064 having a cv error of 2.716745953633525\n"
     ]
    }
   ],
   "source": [
    "min_five_fold = min(five_fold_error)\n",
    "min_ten_fold = min(ten_fold_error)\n",
    "print(\"Lambda value proposed by 5-fold cv is: \" + str(lambd_seq[five_fold_error.index(min_five_fold)]) + \n",
    "      \" having a cv error of \" + str(min_five_fold))\n",
    "print(\"Lambda value proposed by 10-fold cv is: \" + str(lambd_seq[ten_fold_error.index(min_ten_fold)]) + \n",
    "      \" having a cv error of \" + str(min_ten_fold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the graph, the `training error` is the only function that seems to increase steadily as `lambda`\n",
    "increases (perhaps even exponentially, if we only consider the scale of this graph). We can also see\n",
    "that the training error is extremely low for small lambda, which is the exact opposite for the other 3\n",
    "errors. For `testing`, 5-fold, and 10-fold errors, the errors originally start off at a very high error rate,\n",
    "but significantly decrease between 0 to 0.2 lambda, then it comes to a gradual increase.\n",
    "While the gap between 10-fold and 5-fold errors didn’t differ by a significant amount, we can see that\n",
    "5-fold error performed consistently better than both both the testing and 10-fold errors. This is due\n",
    "to the fact that when `lambda` increases in ridge regression, the variance decreases but the bias will in\n",
    "turn increase"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
